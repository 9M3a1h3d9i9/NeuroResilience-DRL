{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOdTGlXdfdJZ/eGbBGoTvS5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/9M3a1h3d9i9/NeuroResilience-DRL/blob/main/NeuroResilience.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. نصب کتابخانه های مورد نیاز (این خط را در کولب اجرا کنید)\n"
      ],
      "metadata": {
        "id": "4VUmqRkFag0y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium stable-baselines3 networkx matplotlib shimmy\n"
      ],
      "metadata": {
        "id": "ONbUYI-faUGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.env_checker import check_env\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VdCYY9DPaWaf",
        "outputId": "375a2aa1-6f28-4265-90d1-6433be010b91"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
            "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
            "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  بخش اول: تعریف محیط گراف (Custom Environment)\n"
      ],
      "metadata": {
        "id": "X1phc8cYaYrg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GraphInterdictionEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    محیط سفارشی برای مسئله کاهش جریان شبکه (Interdiction).\n",
        "    هدف عامل: حذف یال‌هایی که بیشترین تاثیر را در کاهش Max-Flow دارند.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_nodes=20, num_edges_to_remove=3):\n",
        "        super(GraphInterdictionEnv, self).__init__()\n",
        "        self.num_nodes = num_nodes\n",
        "        self.budget = num_edges_to_remove\n",
        "\n",
        "        # ساخت گراف اولیه (مدل Barabasi-Albert برای شبیه سازی شبکه های واقعی)\n",
        "        self.base_graph = nx.barabasi_albert_graph(num_nodes, 3)\n",
        "        # اختصاص ظرفیت به یال ها (فعلا همه 1)\n",
        "        for u, v in self.base_graph.edges():\n",
        "            self.base_graph[u][v]['capacity'] = 1.0\n",
        "\n",
        "        self.source = 0\n",
        "        self.target = num_nodes - 1\n",
        "\n",
        "        # تعریف فضای کنش (Action Space): انتخاب یکی از یال ها برای حذف\n",
        "        # تعداد یال ها ممکن است تغییر کند، پس حد بالا را در نظر می گیریم\n",
        "        self.max_edges = self.num_nodes * (self.num_nodes - 1) // 2\n",
        "        self.action_space = spaces.Discrete(self.max_edges)\n",
        "\n",
        "        # تعریف فضای حالت (Observation Space): ماتریس مجاورت گراف\n",
        "        self.observation_space = spaces.Box(low=0, high=1, shape=(num_nodes, num_nodes), dtype=np.float32)\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        super().reset(seed=seed)\n",
        "        # بازیابی گراف اولیه\n",
        "        self.current_graph = self.base_graph.copy()\n",
        "        self.steps_taken = 0\n",
        "\n",
        "        # لیست یال ها برای نگاشت Action به Edge\n",
        "        self.edge_list = list(self.current_graph.edges())\n",
        "\n",
        "        obs = nx.to_numpy_array(self.current_graph).astype(np.float32) # Cast to float32\n",
        "        return obs, {}\n",
        "\n",
        "    def step(self, action):\n",
        "        # بررسی معتبر بودن کنش\n",
        "        if action < len(self.edge_list):\n",
        "            edge_to_remove = self.edge_list[action]\n",
        "\n",
        "            if self.current_graph.has_edge(*edge_to_remove):\n",
        "                self.current_graph.remove_edge(*edge_to_remove)\n",
        "\n",
        "        # محاسبه پاداش: هرچقدر Max-Flow کمتر شود، پاداش بیشتر است\n",
        "        # ما می خواهیم عامل یاد بگیرد جریان را \"قطع\" کند.\n",
        "        try:\n",
        "            flow_value = nx.maximum_flow_value(self.current_graph, self.source, self.target)\n",
        "        except:\n",
        "            flow_value = 0 # اگر مسیر قطع شده باشد\n",
        "\n",
        "        # پاداش = منفی جریان (چون RL می خواهد پاداش را زیاد کند، پس جریان را کم می کند)\n",
        "        reward = -flow_value\n",
        "\n",
        "        self.steps_taken += 1\n",
        "        terminated = self.steps_taken >= self.budget\n",
        "        truncated = False\n",
        "\n",
        "        obs = nx.to_numpy_array(self.current_graph).astype(np.float32) # Cast to float32\n",
        "        return obs, reward, terminated, truncated, {}"
      ],
      "metadata": {
        "id": "o5JL--M4aeQQ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# بخش دوم: آموزش مدل (Training)"
      ],
      "metadata": {
        "id": "cTyzfjrWak-I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ساخت محیط\n"
      ],
      "metadata": {
        "id": "rkB3_2nQancY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = GraphInterdictionEnv(num_nodes=30, num_edges_to_remove=5)\n",
        "\n",
        "# چک کردن استاندارد بودن محیط\n",
        "check_env(env)\n",
        "\n",
        "# تعریف مدل PPO (عامل هوشمند)\n",
        "model = PPO(\"MlpPolicy\", env, verbose=1, learning_rate=0.001)\n",
        "\n",
        "print(\"شروع آموزش عامل هوشمند (این مرحله چند دقیقه طول می کشد)...\")\n",
        "# آموزش به اندازه 20,000 گام\n",
        "model.learn(total_timesteps=20000)\n",
        "\n",
        "# --- بخش سوم: تست و ارزیابی ---\n",
        "print(\"\\nآموزش تمام شد. حالا بیایید تست کنیم!\")\n",
        "\n",
        "obs, _ = env.reset()\n",
        "total_reward = 0\n",
        "print(f\"تعداد گره ها: {env.num_nodes}, هدف: حذف 5 یال حیاتی\")\n",
        "\n",
        "for i in range(5):\n",
        "    action, _states = model.predict(obs)\n",
        "    obs, reward, done, _, _ = env.step(action)\n",
        "    total_reward += reward\n",
        "    print(f\"گام {i+1}: عامل یال شماره {action} را حذف کرد. پاداش لحظه ای: {reward}\")\n",
        "\n",
        "print(f\"\\nعملکرد نهایی عامل: {total_reward} (هرچه به 0 نزدیکتر باشد یعنی جریان را بهتر قطع کرده)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOu01GTUasKH",
        "outputId": "562ca1af-3919-47c0-edd8-a7d31cdb1cfc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/stable_baselines3/common/env_checker.py:316: UserWarning: Your observation  has an unconventional shape (neither an image, nor a 1D vector). We recommend you to flatten the observation to have only a 1D vector or use a custom policy to properly process the data.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "شروع آموزش عامل هوشمند (این مرحله چند دقیقه طول می کشد)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 5        |\n",
            "|    ep_rew_mean     | -14.8    |\n",
            "| time/              |          |\n",
            "|    fps             | 311      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 6        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 5           |\n",
            "|    ep_rew_mean          | -14.9       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 283         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 14          |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.029722467 |\n",
            "|    clip_fraction        | 0.46        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -6.05       |\n",
            "|    explained_variance   | 9.55e-05    |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 5.77        |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.0434     |\n",
            "|    value_loss           | 14.5        |\n",
            "-----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 5           |\n",
            "|    ep_rew_mean          | -14.9       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 260         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 23          |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.027576786 |\n",
            "|    clip_fraction        | 0.423       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -6.02       |\n",
            "|    explained_variance   | 0.000478    |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 9.51        |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.0378     |\n",
            "|    value_loss           | 17.5        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 5           |\n",
            "|    ep_rew_mean          | -14.8       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 253         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 32          |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.026897559 |\n",
            "|    clip_fraction        | 0.441       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -5.98       |\n",
            "|    explained_variance   | 8.85e-05    |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 10.1        |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.0417     |\n",
            "|    value_loss           | 18          |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 5           |\n",
            "|    ep_rew_mean          | -14.7       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 255         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 40          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.025492158 |\n",
            "|    clip_fraction        | 0.381       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -5.96       |\n",
            "|    explained_variance   | 0.000142    |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 9.99        |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.0365     |\n",
            "|    value_loss           | 18          |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 5           |\n",
            "|    ep_rew_mean          | -14.5       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 249         |\n",
            "|    iterations           | 6           |\n",
            "|    time_elapsed         | 49          |\n",
            "|    total_timesteps      | 12288       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.030831095 |\n",
            "|    clip_fraction        | 0.427       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -5.93       |\n",
            "|    explained_variance   | 0.000817    |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 10.6        |\n",
            "|    n_updates            | 50          |\n",
            "|    policy_gradient_loss | -0.0407     |\n",
            "|    value_loss           | 17.8        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 5           |\n",
            "|    ep_rew_mean          | -14.4       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 246         |\n",
            "|    iterations           | 7           |\n",
            "|    time_elapsed         | 58          |\n",
            "|    total_timesteps      | 14336       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.025023568 |\n",
            "|    clip_fraction        | 0.421       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -5.89       |\n",
            "|    explained_variance   | 0.000258    |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 8.13        |\n",
            "|    n_updates            | 60          |\n",
            "|    policy_gradient_loss | -0.0472     |\n",
            "|    value_loss           | 17.8        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 5           |\n",
            "|    ep_rew_mean          | -14         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 247         |\n",
            "|    iterations           | 8           |\n",
            "|    time_elapsed         | 66          |\n",
            "|    total_timesteps      | 16384       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020354819 |\n",
            "|    clip_fraction        | 0.305       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -5.84       |\n",
            "|    explained_variance   | 0.0936      |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 8.57        |\n",
            "|    n_updates            | 70          |\n",
            "|    policy_gradient_loss | -0.0461     |\n",
            "|    value_loss           | 16.2        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 5           |\n",
            "|    ep_rew_mean          | -14.2       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 247         |\n",
            "|    iterations           | 9           |\n",
            "|    time_elapsed         | 74          |\n",
            "|    total_timesteps      | 18432       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.024447761 |\n",
            "|    clip_fraction        | 0.38        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -5.8        |\n",
            "|    explained_variance   | 0.17        |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 8.81        |\n",
            "|    n_updates            | 80          |\n",
            "|    policy_gradient_loss | -0.0451     |\n",
            "|    value_loss           | 14.8        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 5           |\n",
            "|    ep_rew_mean          | -13.9       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 244         |\n",
            "|    iterations           | 10          |\n",
            "|    time_elapsed         | 83          |\n",
            "|    total_timesteps      | 20480       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.025783429 |\n",
            "|    clip_fraction        | 0.414       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -5.72       |\n",
            "|    explained_variance   | 0.195       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 10.1        |\n",
            "|    n_updates            | 90          |\n",
            "|    policy_gradient_loss | -0.0495     |\n",
            "|    value_loss           | 14.8        |\n",
            "-----------------------------------------\n",
            "\n",
            "آموزش تمام شد. حالا بیایید تست کنیم!\n",
            "تعداد گره ها: 30, هدف: حذف 5 یال حیاتی\n",
            "گام 1: عامل یال شماره 116 را حذف کرد. پاداش لحظه ای: -3.0\n",
            "گام 2: عامل یال شماره 427 را حذف کرد. پاداش لحظه ای: -3.0\n",
            "گام 3: عامل یال شماره 69 را حذف کرد. پاداش لحظه ای: -2.0\n",
            "گام 4: عامل یال شماره 79 را حذف کرد. پاداش لحظه ای: -2.0\n",
            "گام 5: عامل یال شماره 16 را حذف کرد. پاداش لحظه ای: -2.0\n",
            "\n",
            "عملکرد نهایی عامل: -12.0 (هرچه به 0 نزدیکتر باشد یعنی جریان را بهتر قطع کرده)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  بخش چهارم: رسم نمودار (اختیاری - نیاز به Callback دارد اما اینجا ساده تست کردیم)\n"
      ],
      "metadata": {
        "id": "87SFED0Faxev"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# برای رسم دقیق نمودار Learning Curve در اجراهای بعدی از Monitor استفاده می کنیم."
      ],
      "metadata": {
        "id": "qFRyhCVma0sg"
      }
    }
  ]
}